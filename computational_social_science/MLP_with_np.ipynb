{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer perceptron\n",
    "\n",
    "- Implementation of a multilayer perceptron \n",
    "- Consisting of multiple fully connected layers with biases each followed (potentially) by an activation function\n",
    "- This implementation is based on the Python package numpy. \n",
    "\n",
    "The MLP is defined by providing the sizes of the layers (including input and output layers) and the activation functions that are applied after each layer. I used vanilla stochastic gradient descent with mini-batches for weight & bias updates.\n",
    "\n",
    "You can find implementations of activation functions below. They can be used in the forward pass but also provide the derivative if `deriv=True` is provided. See more details in the comment in a).\n",
    "\n",
    "### a) forward\n",
    "\n",
    "A function `forward` that takes an input array that should then be passed through the entire network. All intermediate layer results are stored in `y`. Thereby also store the input (raw input) and the output layer.\n",
    "The forward pass goes as follows\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588); padding:10px 0;font-family:monospace;\">\n",
    "$y_0 = x$ <br>\n",
    "for $k=0..$ do<br>\n",
    "&emsp;    $z_{k} =  W_k y_{k} + b_k$ <br>\n",
    "&emsp;    $y_{k+1} = g_{k}(z_{k})$\n",
    "</div>\n",
    "\n",
    "\n",
    "Where $W$ is the weight matrix, $b$ is the bias vector and $g_{k}$ the activation function at layer $k$. \n",
    "\n",
    "The variable $z_k$ is not stored but is here for illustration. In general one potentially would have to store it too but we do not need it for the activation functions sigmoid and relu. They have the special property that we can write those  as $g'(x) = G(g(x))$ for some function $G$.\n",
    "\n",
    "### b) back propagation\n",
    "\n",
    "The backpropagation is computed as:\n",
    "\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588); padding:10px 0;font-family:monospace;\">\n",
    "$h \\leftarrow \\frac{\\partial L}{\\partial y}$<br>\n",
    "for $k = l, l-1,... 1$ do:<br>\n",
    "&emsp; $h \\leftarrow \\frac{\\partial L}{\\partial z_k} = h \\odot g' _{k-1}(z_{k-1}) = h \\odot G_{k-1}(y_k)$<br>\n",
    "&emsp; $\\frac{\\partial L}{\\partial W_{k-1}} = y_{k-1} \\otimes h$       &emsp;# use +=<br>\n",
    "&emsp; $\\frac{\\partial L}{\\partial b_{k-1}} = h$     &emsp;&emsp; # use += <br>\n",
    "&emsp; $h \\leftarrow \\frac{\\partial L}{\\partial y_k} = h \\cdot W_{k-1}$\n",
    "</div>\n",
    "\n",
    "\n",
    "Where $\\odot$ is the element wise product, $\\otimes$ is the outer product and $\\cdot$ the dot product. It assumes that a forward step has previously happened and has set the values of `results` accordingly. To keep it simple, this function is intended for one instance only but we want to employ it in a mini batch scenario.\n",
    "\n",
    "### c) tanh activation (1)\n",
    "\n",
    "The tanh activation function is implemented in a way such that they can also be used in your MLP. See e.g. https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions\n",
    "\n",
    "\n",
    "### d) update (4)\n",
    "\n",
    "A function `update` updates the weights and biases for all layers based on a mini-batch (given as lists of input arrays `xs` and desired targets `ts`), loss function `loss_func` and and learning rate $\\mu$. It therefore first zeros previous gradients (implement the `zero_grad` function), then does a forward and backwards pass for each training instance and finally updates the weights and biases using the learning rate. \n",
    "\n",
    "This is a simplified process. In pratice, all of the instances in the mini-batch would be passed forward and backwards simultaneously, we do not do it here.\n",
    "\n",
    "### e) Evaluation functions (3)\n",
    "\n",
    "The functions `calc_accuracy` and `calc_loss` calculate the classification accuracy and the *average* loss over the provided examples `xs` and `ts`.\n",
    "\n",
    "\n",
    "### f) Preparing real data (2)\n",
    "A function `top_n_words(sentences, n)` returns a list of the most frequently used words for a corpus. You can assume that the function is tested such that there is no tie.\n",
    "\n",
    "A function `to_hot_encoding(sentences, top_words)` encodes the document as a numpy array of type float. It has the same length as `top_words` and indicates the presence or abscence of that particular word by 1 = present, 0 = absent. This is a bag-of-words model.\n",
    "\n",
    "The sentences are provided as a list of list of tokens. \n",
    "\n",
    "### g) fiddle with hyper parameters (2)\n",
    "\n",
    "Hyperparameters are found\n",
    "```\n",
    "my_seed = 1\n",
    "my_sizes = (128, ...,2)\n",
    "my_activations = (..., sigmoid)\n",
    "my_epochs = 1\n",
    "my_chunks = 1\n",
    "```\n",
    "such that you achieve >90% accuracy on the training set and >75% accuracy on the test set using the provided training loop. Available activation functions were limited to `relu`, `identity`, `tanh` and `sigmoid`. The training time was limited to a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, sizes, activations):\n",
    "        assert len(activations) == (len(sizes)-1)\n",
    "        self.activation_functions = tuple(activations)\n",
    "        \n",
    "        # init weights, biases and temporary results\n",
    "        self.weights = tuple(((np.random.random_sample((sizes[i], sizes[i+1]))-0.5) for i in range(len(sizes)-1)))\n",
    "        self.biases = tuple((np.random.random_sample(sizes[i+1])-0.5) for i in range(len(sizes)-1))\n",
    "        self.y = tuple(np.empty(sizes[i]) for i in range(len(sizes)))\n",
    "        \n",
    "        # init gradients\n",
    "        self.gradients = tuple((np.zeros(arr.shape) for arr in self.weights))\n",
    "        self.biases_gradients = tuple((np.zeros(arr.shape) for arr in self.biases))\n",
    "\n",
    "\n",
    "    def forward(self, input_arr):\n",
    "        # input_array is a numpy array,\n",
    "        # this function returns nothing\n",
    "        \n",
    "        # initialize\n",
    "        k = 0 \n",
    "        self.y[k][:] = input_arr\n",
    "        layer = input_arr\n",
    "        \n",
    "        for activation in self.activation_functions:\n",
    "            layer = activation(self.weights[k].T.dot(layer) + self.biases[k])\n",
    "            k += 1\n",
    "            self.y[k][:] = layer\n",
    "        \n",
    "        # pass\n",
    "\n",
    "\n",
    "    def back_prop(self, t, loss_func):\n",
    "        # t : target label == desired form of the final label\n",
    "        # loss_func: a loss function see squared loss below\n",
    "        #pass\n",
    "\n",
    "        k = len(self.activation_functions) \n",
    "        hah = loss_func(self.y[k], t, True) \n",
    "        \n",
    "        # calculate derivatives for activation functions\n",
    "        for activation in reversed(self.activation_functions):\n",
    "            hah = np.multiply(hah, activation(self.y[k], deriv=True))\n",
    "            self.gradients[k-1][:] += np.outer(self.y[k-1], hah)\n",
    "            self.biases_gradients[k-1][:] += hah\n",
    "            hah = hah.dot(self.weights[k-1].T)\n",
    "            k -= 1\n",
    "\n",
    "    def zero_grad(self):\n",
    "        #pass\n",
    "        #self.gradients = tuple((np.zeros(w.shape) for w in self.weights))\n",
    "        #self.biases_gradients = tuple((np.zeros(w.shape) for w in self.biases))\n",
    "        \n",
    "        for i in range(len(self.gradients)):\n",
    "            self.gradients[i][:] = np.zeros(self.weights[i].shape)\n",
    "        \n",
    "        for i in range(len(self.biases_gradients)):\n",
    "            self.biases_gradients[i][:] = np.zeros(self.biases[i].shape)\n",
    "        '''\n",
    "        self.gradients[:][:] = tuple((np.zeros(w.shape) for w in self.weights))\n",
    "        '''\n",
    "        \n",
    "    def update(self, xs, ts, loss_func, mu):\n",
    "        # xs: list of numpy arrays (inputs)\n",
    "        # ts: list of numpy arrays (desired output)\n",
    "        # loss_func: function, a loss function see squared loss below\n",
    "        # mu: float, learning rate\n",
    "        \n",
    "        assert len(ts) == len(xs)\n",
    "        \n",
    "        self.zero_grad()\n",
    "\n",
    "        for idx in range(len(xs)):\n",
    "            self.forward(xs[idx])\n",
    "            self.back_prop(ts[idx], loss_func)\n",
    "            self.weights -= np.multiply(mu, self.gradients)\n",
    "            self.biases -= np.multiply(mu, self.biases_gradients)\n",
    "        \n",
    "        \n",
    "    def calc_accuracy(self, xs, ts):\n",
    "        # xs: list of numpy arrays (inputs)\n",
    "        # ts: list of numpy arrays (desired output)\n",
    "        same = 0\n",
    "        diff = 0\n",
    "        \n",
    "        for idx in range(len(xs)):\n",
    "            self.forward(xs[idx])\n",
    "            if np.argmax(self.y[-1]) == np.argmax(ts[idx]):\n",
    "                same += 1\n",
    "            else:\n",
    "                diff +=1\n",
    "            \n",
    "        acc = round(same / (same+diff), 3)\n",
    "        return acc\n",
    "    \n",
    "        #return 0.0\n",
    "    \n",
    "    def calc_loss(self, xs, ts, loss_func):\n",
    "        # xs: list of numpy arrays (inputs)\n",
    "        # ts: list of numpy arrays (desired output)\n",
    "        # loss_func: function, a loss function see squared loss below\n",
    "        loss = 0\n",
    "        for idx in range(len(xs)):\n",
    "            self.forward(xs[idx])\n",
    "            loss += loss_func(self.y[-1], ts[idx])\n",
    "        return loss/len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "# if deriv=True they act like the function capital G as explained above\n",
    "#   that is they assume that the input is not x but f(x)\n",
    "#   take inspiration from the other activation functions\n",
    "\n",
    "def identity(x, deriv=False):\n",
    "    if deriv == True:\n",
    "        return 1\n",
    "    return x\n",
    "\n",
    "\n",
    "def relu(x, deriv=False):\n",
    "    if deriv == True:\n",
    "        out = np.zeros(x.shape)\n",
    "        out[x>0]=x[x>0]\n",
    "        return out\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def sigmoid(x, deriv=False):\n",
    "    if deriv == True:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Examples tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take inspiration from the other activation functions implemented above\n",
    "\n",
    "def tanh(x, deriv=False):\n",
    "    if deriv == True:\n",
    "        return 1 - np.power(x, 2)\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "    #return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76159416 0.96402758 0.99505475]\n",
      "[ 0 -3 -8]\n",
      "[  -3  -24 -120]\n"
     ]
    }
   ],
   "source": [
    "print(tanh(np.array([1, 2, 3])))\n",
    "print(tanh(np.array([1, 2, 3]), deriv=True))\n",
    "print(tanh(np.array([2, 5, 11]), deriv=True))\n",
    "\n",
    "#[0.76159416 0.96402758 0.99505475]\n",
    "#[ 0 -3 -8]\n",
    "#[  -3  -24 -120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def squared_loss(x, t, deriv=False):\n",
    "    if deriv:\n",
    "        return x - t\n",
    "    return 0.5*np.sum((np.square(x-t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Examples forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0.]\n",
      "[0.         0.28896105 0.4080556  0.43338515]\n",
      "[0.         0.03587933 0.        ]\n",
      "[0.48869641 0.5991624 ]\n"
     ]
    }
   ],
   "source": [
    "in1 = np.array([0,0,1,0,0], dtype=float)\n",
    "np.random.seed(1)\n",
    "\n",
    "activs = [relu, relu, sigmoid]\n",
    "myNN = MLP((5,4,3,2), activs)\n",
    "y0, y1, y2, y3 = myNN.y\n",
    "\n",
    "myNN.forward(in1)\n",
    "print(myNN.y[0])\n",
    "print(myNN.y[1])\n",
    "print(myNN.y[2])\n",
    "print(myNN.y[-1])\n",
    "\n",
    "# check that we have done everything in place\n",
    "assert myNN.y[0] is y0\n",
    "assert myNN.y[1] is y1\n",
    "assert myNN.y[2] is y2\n",
    "assert myNN.y[-1] is y3\n",
    "\n",
    "#[0. 0. 1. 0. 0.]\n",
    "#[0.         0.28896105 0.4080556  0.43338515]\n",
    "#[0.         0.03587933 0.        ]\n",
    "#[0.48869641 0.5991624 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 0. 0.]\n",
      "[0.         0.50928554 0.         0.23571773]\n",
      "[0.         0.38629211 0.        ]\n",
      "[0.50550331 0.58354196]\n"
     ]
    }
   ],
   "source": [
    "in2 = np.array([1,0,1,0,0], dtype=float)\n",
    "myNN.forward(in2)\n",
    "\n",
    "# check that we have done everything in place\n",
    "assert myNN.y[0] is y0\n",
    "assert myNN.y[1] is y1\n",
    "assert myNN.y[2] is y2\n",
    "assert myNN.y[-1] is y3\n",
    "\n",
    "print(myNN.y[0])\n",
    "print(myNN.y[1])\n",
    "print(myNN.y[2])\n",
    "print(myNN.y[-1])\n",
    "#[1. 0. 1. 0. 0.]\n",
    "#[0.         0.50928554 0.         0.23571773]\n",
    "#[0.         0.38629211 0.        ]\n",
    "#[0.50550331 0.58354196]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Examples backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-0.082978  ,  0.22032449, -0.49988563, -0.19766743],\n",
      "       [-0.35324411, -0.40766141, -0.31373979, -0.15443927],\n",
      "       [-0.10323253,  0.03881673, -0.08080549,  0.1852195 ],\n",
      "       [-0.29554775,  0.37811744, -0.47261241,  0.17046751],\n",
      "       [-0.0826952 ,  0.05868983, -0.35961306, -0.30189851]]), array([[ 0.30074457,  0.46826158, -0.18657582],\n",
      "       [ 0.19232262,  0.37638915,  0.39460666],\n",
      "       [-0.41495579, -0.46094522, -0.33016958],\n",
      "       [ 0.3781425 , -0.40165317, -0.07889237]]), array([[ 0.45788953,  0.03316528],\n",
      "       [ 0.19187711, -0.18448437],\n",
      "       [ 0.18650093,  0.33462567]]))\n",
      "(array([-0.48171172,  0.25014431,  0.48886109,  0.24816565]), array([-0.21955601,  0.28927933, -0.39677399]), array([-0.05210647,  0.4085955 ]))\n",
      "(array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.00019926,  0.00034459,  0.00031891],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ]]), array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.00052939,  0.        ],\n",
      "       [ 0.        , -0.00074758,  0.        ],\n",
      "       [ 0.        , -0.00079398,  0.        ]]), array([[ 0.        ,  0.        ],\n",
      "       [-0.00458396,  0.005163  ],\n",
      "       [ 0.        ,  0.        ]]))\n",
      "(array([ 0.        , -0.00019926,  0.00034459,  0.00031891]), array([ 0.        , -0.00183205,  0.        ]), array([-0.12776057,  0.14389893]))\n"
     ]
    }
   ],
   "source": [
    "myNN.zero_grad()\n",
    "myNN.forward(in1)\n",
    "w0, w1, w2 = myNN.gradients\n",
    "b0, b1, b2 = myNN.biases_gradients\n",
    "myNN.back_prop(np.array([1,0]), squared_loss)\n",
    "\n",
    "# untouched\n",
    "print(myNN.weights)\n",
    "print(myNN.biases)\n",
    "\n",
    "# changed\n",
    "print(myNN.gradients)\n",
    "print(myNN.biases_gradients)\n",
    "\n",
    "# check that we have done everything in place\n",
    "assert w0 is myNN.gradients[0]\n",
    "assert w1 is myNN.gradients[1]\n",
    "assert w2 is myNN.gradients[2]\n",
    "assert b0 is myNN.biases_gradients[0]\n",
    "assert b1 is myNN.biases_gradients[1]\n",
    "assert b2 is myNN.biases_gradients[2]\n",
    "\n",
    "# (array([[-0.082978  ,  0.22032449, -0.49988563, -0.19766743],\n",
    "#       [-0.35324411, -0.40766141, -0.31373979, -0.15443927],\n",
    "#       [-0.10323253,  0.03881673, -0.08080549,  0.1852195 ],\n",
    "#       [-0.29554775,  0.37811744, -0.47261241,  0.17046751],\n",
    "#       [-0.0826952 ,  0.05868983, -0.35961306, -0.30189851]]),\n",
    "#        array([[ 0.30074457,  0.46826158, -0.18657582],\n",
    "#       [ 0.19232262,  0.37638915,  0.39460666],\n",
    "#       [-0.41495579, -0.46094522, -0.33016958],\n",
    "#       [ 0.3781425 , -0.40165317, -0.07889237]]),\n",
    "#        array([[ 0.45788953,  0.03316528],\n",
    "#       [ 0.19187711, -0.18448437],\n",
    "#       [ 0.18650093,  0.33462567]]))\n",
    "#(array([-0.48171172,  0.25014431,  0.48886109,  0.24816565]), array([-0.21955601,  0.28927933, -0.39677399]), array([-0.05210647,  0.4085955 ]))\n",
    "#(array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
    "#       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
    "#       [ 0.        , -0.00019926,  0.00034459,  0.00031891],\n",
    "#       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
    "#       [ 0.        ,  0.        ,  0.        ,  0.        ]]), array([[ 0.        ,  0.        ,  0.        ],\n",
    "#       [ 0.        , -0.00052939,  0.        ],\n",
    "#       [ 0.        , -0.00074758,  0.        ],\n",
    "#       [ 0.        , -0.00079398,  0.        ]]), array([[ 0.        ,  0.        ],\n",
    "#       [-0.00458396,  0.005163  ],\n",
    "#       [ 0.        ,  0.        ]]))\n",
    "#(array([ 0.        , -0.00019926,  0.00034459,  0.00031891]), array([ 0.        , -0.00183205,  0.        ]), array([-0.12776057,  0.14389893]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]), array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]]), array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]]))\n",
      "(array([0., 0., 0., 0.]), array([0., 0., 0.]), array([0., 0.]))\n",
      "---0---\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "------\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "---1---\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "------\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "--2----\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "------\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "myNN.zero_grad()\n",
    "print(myNN.gradients)\n",
    "print(myNN.biases_gradients)\n",
    "print('---0---')\n",
    "# check that we have done everything in place\n",
    "print(w0)\n",
    "#print(np.shape(w0))\n",
    "print('------')\n",
    "#print(np.shape(myNN.gradients[0]))\n",
    "print(myNN.gradients[0])\n",
    "print('---1---')\n",
    "# check that we have done everything in place\n",
    "print(w1)\n",
    "#print(np.shape(w1))\n",
    "print('------')\n",
    "#print(np.shape(myNN.gradients[1]))\n",
    "print(myNN.gradients[1])\n",
    "print('--2----')\n",
    "# check that we have done everything in place\n",
    "print(w2)\n",
    "#print(np.shape(w2))\n",
    "print('------')\n",
    "#print(np.shape(myNN.gradients[2]))\n",
    "print(myNN.gradients[2])\n",
    "assert w0 is myNN.gradients[0]\n",
    "assert w1 is myNN.gradients[1]\n",
    "assert w2 is myNN.gradients[2]\n",
    "assert b0 is myNN.biases_gradients[0]\n",
    "assert b1 is myNN.biases_gradients[1]\n",
    "assert b2 is myNN.biases_gradients[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### b) Examples backward with two training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-0.082978  ,  0.22032449, -0.49988563, -0.19766743],\n",
      "       [-0.35324411, -0.40766141, -0.31373979, -0.15443927],\n",
      "       [-0.10323253,  0.03881673, -0.08080549,  0.1852195 ],\n",
      "       [-0.29554775,  0.37811744, -0.47261241,  0.17046751],\n",
      "       [-0.0826952 ,  0.05868983, -0.35961306, -0.30189851]]), array([[ 0.30074457,  0.46826158, -0.18657582],\n",
      "       [ 0.19232262,  0.37638915,  0.39460666],\n",
      "       [-0.41495579, -0.46094522, -0.33016958],\n",
      "       [ 0.3781425 , -0.40165317, -0.07889237]]), array([[ 0.45788953,  0.03316528],\n",
      "       [ 0.19187711, -0.18448437],\n",
      "       [ 0.18650093,  0.33462567]]))\n",
      "(array([-0.48171172,  0.25014431,  0.48886109,  0.24816565]), array([-0.21955601,  0.28927933, -0.39677399]), array([-0.05210647,  0.4085955 ]))\n",
      "(array([[ 0.        , -0.00037368,  0.        ,  0.00018456],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.00057294,  0.00034459,  0.00050347],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ]]), array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -0.0015222 ,  0.        ],\n",
      "       [ 0.        , -0.00074758,  0.        ],\n",
      "       [ 0.        , -0.0012535 ,  0.        ]]), array([[ 0.        ,  0.        ],\n",
      "       [-0.05233322, -0.03393283],\n",
      "       [ 0.        ,  0.        ]]))\n",
      "(array([ 0.        , -0.00057294,  0.00034459,  0.00050347]), array([ 0.        , -0.00378147,  0.        ]), array([-0.25136976,  0.04269099]))\n"
     ]
    }
   ],
   "source": [
    "myNN.zero_grad()\n",
    "\n",
    "# accumulates gradients over two examples\n",
    "myNN.forward(in1)\n",
    "myNN.back_prop(np.array([1,0]), squared_loss)\n",
    "myNN.forward(in2)\n",
    "myNN.back_prop(np.array([1,1]), squared_loss)\n",
    "\n",
    "\n",
    "# untouched\n",
    "print(myNN.weights)\n",
    "print(myNN.biases)\n",
    "\n",
    "# changed\n",
    "print(myNN.gradients)\n",
    "print(myNN.biases_gradients)\n",
    "\n",
    "#(array([[-0.082978  ,  0.22032449, -0.49988563, -0.19766743],\n",
    "#       [-0.35324411, -0.40766141, -0.31373979, -0.15443927],\n",
    "#       [-0.10323253,  0.03881673, -0.08080549,  0.1852195 ],\n",
    "#       [-0.29554775,  0.37811744, -0.47261241,  0.17046751],\n",
    "#       [-0.0826952 ,  0.05868983, -0.35961306, -0.30189851]]), array([[ 0.30074457,  0.46826158, -0.18657582],\n",
    "#       [ 0.19232262,  0.37638915,  0.39460666],\n",
    "#       [-0.41495579, -0.46094522, -0.33016958],\n",
    "#       [ 0.3781425 , -0.40165317, -0.07889237]]), array([[ 0.45788953,  0.03316528],\n",
    "#       [ 0.19187711, -0.18448437],\n",
    "#       [ 0.18650093,  0.33462567]]))\n",
    "#(array([-0.48171172,  0.25014431,  0.48886109,  0.24816565]), array([-0.21955601,  0.28927933, -0.39677399]), array([-0.05210647,  0.4085955 ]))\n",
    "\n",
    "# changed\n",
    "#(array([[ 0.        , -0.00037368,  0.        ,  0.00018456],\n",
    "#       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
    "#       [ 0.        , -0.00057294,  0.00034459,  0.00050347],\n",
    "#       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
    "#       [ 0.        ,  0.        ,  0.        ,  0.        ]]), array([[ 0.        ,  0.        ,  0.        ],\n",
    "#       [ 0.        , -0.0015222 ,  0.        ],\n",
    "#       [ 0.        , -0.00074758,  0.        ],\n",
    "#       [ 0.        , -0.0012535 ,  0.        ]]), array([[ 0.        ,  0.        ],\n",
    "#       [-0.05233322, -0.03393283],\n",
    "#       [ 0.        ,  0.        ]]))\n",
    "#(array([ 0.        , -0.00057294,  0.00034459,  0.00050347]),\n",
    "# array([ 0.        , -0.00378147,  0.        ]),\n",
    "# array([-0.25136976,  0.04269099]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Examples for update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that generates some dummy training dataset\n",
    "def generate_training(n):\n",
    "    xs=[]\n",
    "    ts=[]\n",
    "    for _ in range(n):\n",
    "        x = np.random.random_sample(10)>0.5\n",
    "        t = (x[1] and x[2]) or x[3]\n",
    "        xs.append(x)\n",
    "        ts.append(np.array([t,~t]))\n",
    "        \n",
    "    return xs, ts\n",
    "    return np.array(xs), np.array(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xs, ts = generate_training(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0.24257539789489851 0.634\n",
      "# 0.23680018014797793 0.634\n",
      "# 0.23486961983067542 0.634\n",
      "# 0.23763274357305297 0.634\n",
      "# 0.23294888559644564 0.634\n",
      "# 0.23122098354943119 0.634\n",
      "# 0.2304775784731769 0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dd/ygnrmj7170l_gqhb4wrx72lw0000gn/T/ipykernel_85765/2791676855.py:76: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.weights -= np.multiply(mu, self.gradients)\n",
      "/var/folders/dd/ygnrmj7170l_gqhb4wrx72lw0000gn/T/ipykernel_85765/2791676855.py:77: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.biases -= np.multiply(mu, self.biases_gradients)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0.22330318362069906 0.634\n",
      "# 0.21893212674055548 0.634\n",
      "# 0.22047076752731185 0.634\n",
      "# 0.20693636742514407 0.634\n",
      "# 0.2552210296963756 0.634\n",
      "# 0.21531657934819837 0.634\n",
      "# 0.20623799899411183 0.634\n",
      "# 0.19881888295769318 0.634\n",
      "# 0.1762144479903395 0.645\n",
      "# 0.22747002449625175 0.634\n",
      "# 0.17239514867314046 0.637\n",
      "# 0.19650407945536824 0.634\n",
      "# 0.14672626964664534 0.701\n",
      "# 0.2563586858259369 0.634\n",
      "# 0.2577118518965085 0.481\n",
      "# 0.13791970821936927 0.788\n",
      "# 0.10604787657737749 0.898\n",
      "# 0.1347146952911298 0.776\n",
      "# 0.09514207936165966 0.878\n",
      "# 0.1155671086863483 0.838\n",
      "# 0.3188538244925079 0.639\n",
      "# 0.18326460835463557 0.718\n",
      "# 0.09064728376910568 0.9\n",
      "# 0.058834700162359595 0.961\n",
      "# 0.2696931580311026 0.681\n",
      "# 0.28060698402327394 0.513\n",
      "# 0.1841940999426835 0.712\n",
      "# 0.07886446244029571 0.916\n",
      "# 0.07334228771389784 0.907\n",
      "# 0.07513961354583581 0.918\n",
      "# 0.06602938077034148 0.923\n",
      "# 0.10503067478055103 0.88\n",
      "# 0.08853239512915334 0.885\n",
      "# 0.07333191690686433 0.907\n",
      "# 0.06291351946098912 0.931\n",
      "# 0.24959051696936257 0.643\n",
      "# 0.2019566583108498 0.7\n",
      "# 0.07751867493805571 0.907\n",
      "# 0.07576157117232775 0.909\n",
      "# 0.07968970445469435 0.9\n",
      "# 0.10639819847229275 0.86\n",
      "# 0.0835356602513392 0.887\n",
      "# 0.09897271471401584 0.885\n",
      "# 0.09319003012681487 0.897\n",
      "# 0.4699717168014235 0.366\n",
      "# 0.4099868148192362 0.366\n",
      "# 0.2883491961790364 0.461\n",
      "# 0.13170562805253544 0.753\n",
      "# 0.20270157524785737 0.648\n",
      "# 0.19481538190196793 0.658\n",
      "# 0.13655193459962955 0.784\n",
      "# 0.14289310811105418 0.727\n",
      "# 0.1232402133947877 0.848\n",
      "# 0.11534986493577856 0.844\n",
      "# 0.11306748364016705 0.847\n",
      "# 0.11528823597949871 0.844\n",
      "# 0.10772284982648161 0.844\n",
      "# 0.10270688319671274 0.854\n",
      "# 0.10222110342728226 0.844\n",
      "# 0.1499808051333198 0.777\n",
      "# 0.11457585279985259 0.839\n",
      "# 0.1090615321880796 0.841\n",
      "# 0.12297012555707709 0.832\n",
      "# 0.2236615705516543 0.636\n",
      "# 0.12463298549312915 0.874\n",
      "# 0.10069559637355399 0.861\n",
      "# 0.10083102896811577 0.861\n",
      "# 0.09509122832329249 0.876\n",
      "# 0.11623212264690533 0.845\n",
      "# 0.103239294672335 0.864\n",
      "# 0.09779810266383544 0.867\n",
      "# 0.09683274906264708 0.87\n",
      "# 0.09573666523156144 0.873\n",
      "# 0.09570577554405528 0.873\n",
      "# 0.10096228024283457 0.854\n",
      "# 0.1002995586322821 0.864\n",
      "# 0.10033025225428846 0.864\n",
      "# 0.09915729655737898 0.862\n",
      "# 0.09915103994826732 0.862\n",
      "# 0.09919253043625287 0.862\n",
      "# 0.09931450222615308 0.864\n",
      "# 0.08975689069339507 0.886\n",
      "# 0.08856447529628143 0.886\n",
      "# 0.08945404639466911 0.886\n",
      "# 0.1248227999626232 0.824\n",
      "# 0.10261450450416458 0.859\n",
      "# 0.10368115034832899 0.859\n",
      "# 0.10617415685304386 0.855\n",
      "# 0.1009968099273253 0.861\n",
      "# 0.10048630972953823 0.864\n",
      "# 0.09166841079968661 0.895\n",
      "# 0.0887060742632759 0.898\n",
      "# 0.11987284985307824 0.862\n",
      "# 0.08826786112549553 0.879\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "myNN = MLP((10, 8, 4, 2), activs)\n",
    "print(\"#\", myNN.calc_loss(xs,ts, squared_loss), myNN.calc_accuracy(xs, ts))\n",
    "\n",
    "for x_chunk, t_chunk in zip(np.array_split(xs,100), np.array_split(ts,100)):\n",
    "    myNN.update(x_chunk, t_chunk, squared_loss, 0.1)\n",
    "    \n",
    "    # evaluate NN on entire dataset\n",
    "    print(\"#\", myNN.calc_loss(xs, ts, squared_loss), myNN.calc_accuracy(xs, ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output from previous cell\n",
    "# 0.24257539789489851 0.634\n",
    "# 0.2411544143560101 0.634\n",
    "# 0.23984965535059738 0.634\n",
    "# 0.23882087470347274 0.634\n",
    "# 0.23843002829715368 0.634\n",
    "# 0.2382914663918927 0.634\n",
    "# 0.23764527725780307 0.634\n",
    "# 0.23666737755900943 0.634\n",
    "# 0.23620884986400037 0.634\n",
    "# 0.23650340311293885 0.634\n",
    "# 0.23616300372820928 0.634\n",
    "# 0.23502874505528465 0.634\n",
    "# 0.23503090772220395 0.634\n",
    "# 0.23429557276477497 0.634\n",
    "# 0.23407573863293182 0.634\n",
    "# 0.2339776485620543 0.634\n",
    "# 0.23348187532872453 0.634\n",
    "# 0.23290833656298032 0.634\n",
    "# 0.23273709472388385 0.634\n",
    "# 0.2325444855821718 0.634\n",
    "# 0.23267973317482651 0.634\n",
    "# 0.23200642490995707 0.634\n",
    "# 0.23170660058816217 0.634\n",
    "# 0.2314284286110461 0.634\n",
    "# 0.2310843786009857 0.634\n",
    "# 0.2308711195328345 0.634\n",
    "# 0.23047087788541018 0.634\n",
    "# 0.23078563724882506 0.634\n",
    "# 0.23010159343696798 0.634\n",
    "# 0.22980399458119968 0.634\n",
    "# 0.2297798677979091 0.634\n",
    "# 0.2293841765404658 0.634\n",
    "# 0.2291655512063339 0.634\n",
    "# 0.22870691228278373 0.634\n",
    "# 0.22817010393724937 0.634\n",
    "# 0.22771610716266258 0.634\n",
    "# 0.22735622344120002 0.634\n",
    "# 0.2268242701172851 0.634\n",
    "# 0.22650100063083706 0.634\n",
    "# 0.22550104232382934 0.634\n",
    "# 0.22509370708150808 0.634\n",
    "# 0.22467802344347212 0.634\n",
    "# 0.2242107007448516 0.634\n",
    "# 0.2233116482328667 0.634\n",
    "# 0.22265497659346642 0.634\n",
    "# 0.22178494678736085 0.634\n",
    "# 0.2213379424960512 0.634\n",
    "# 0.21990383266003793 0.634\n",
    "# 0.21838496173099078 0.634\n",
    "# 0.21753192292293697 0.634\n",
    "# 0.21669299576563952 0.634\n",
    "# 0.21663406404375596 0.634\n",
    "# 0.2140600877981478 0.634\n",
    "# 0.21325662957467928 0.634\n",
    "# 0.21282023805393604 0.634\n",
    "# 0.21274278318993375 0.634\n",
    "# 0.21016631292648913 0.634\n",
    "# 0.20932144343716885 0.634\n",
    "# 0.21282663190076195 0.634\n",
    "# 0.21085248695856348 0.634\n",
    "# 0.2091929895648754 0.634\n",
    "# 0.2040259930808181 0.634\n",
    "# 0.20830483920311055 0.634\n",
    "# 0.20110187393197654 0.634\n",
    "# 0.19921054775576258 0.634\n",
    "# 0.19614748720352942 0.634\n",
    "# 0.19579048879561475 0.634\n",
    "# 0.19344528532212424 0.634\n",
    "# 0.20045251332451106 0.634\n",
    "# 0.18825747694914757 0.634\n",
    "# 0.18576924889914842 0.634\n",
    "# 0.18411287775658522 0.634\n",
    "# 0.18302615584253373 0.634\n",
    "# 0.18454763429228072 0.634\n",
    "# 0.17626642470559317 0.635\n",
    "# 0.17469065098247072 0.679\n",
    "# 0.1719378298327115 0.661\n",
    "# 0.17605269372506277 0.637\n",
    "# 0.19701745527854836 0.634\n",
    "# 0.1608919682268122 0.716\n",
    "# 0.16342926706599922 0.651\n",
    "# 0.19137490242816632 0.938\n",
    "# 0.16993445388782424 0.662\n",
    "# 0.17335348387678517 0.65\n",
    "# 0.15162184456043243 0.682\n",
    "# 0.14235256482453107 0.807\n",
    "# 0.1512034857104882 0.69\n",
    "# 0.15666428550872563 0.681\n",
    "# 0.2567495441627176 0.634\n",
    "# 0.15486582586192096 0.95\n",
    "# 0.19636555320933272 0.652\n",
    "# 0.18016546272626016 0.84\n",
    "# 0.128967559491965 0.742\n",
    "# 0.13573770765700863 0.862\n",
    "# 0.09399810441538305 0.921\n",
    "# 0.09851463017361554 0.903\n",
    "# 0.11583907110986205 0.943\n",
    "# 0.32636879914274497 0.634\n",
    "# 0.2195528529634276 0.681\n",
    "# 0.2232071483693024 0.676\n",
    "# 0.08403512410215158 0.976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "X=None\n",
    "with open('news_sports_train_X.p', 'rb') as f:\n",
    "    X=pickle.load(f)\n",
    "    \n",
    "X_test=None\n",
    "with open('news_sports_test_X.p', 'rb') as f:\n",
    "    X_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=None\n",
    "with open('news_sports_train_y.p', 'rb') as f:\n",
    "    y=pickle.load(f)\n",
    "    \n",
    "y_test=None\n",
    "with open('news_sports_test_y.p', 'rb') as f:\n",
    "    y_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare targets\n",
    "def get_ts(labels):\n",
    "    ts = []\n",
    "    for label in labels:\n",
    "        l = bool(label)\n",
    "        ts.append(np.array([not l, l], dtype=bool))\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = get_ts(y)\n",
    "ts_test = get_ts(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Implement top_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "def top_n_words(sentences : List[List[str]], n:int) -> List[str]:\n",
    "    return [x[0] for x in Counter(list(chain(*sentences))).most_common(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '--', 'the', '>', '.', ':', ')', '(', 'to', 'a']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words(X,10)\n",
    "# [',', '--', 'the', '>', '.', ':', ')', '(', 'to', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " 'the',\n",
       " '>',\n",
       " '.',\n",
       " '--',\n",
       " ':',\n",
       " ')',\n",
       " 'to',\n",
       " '(',\n",
       " 'a',\n",
       " 'in',\n",
       " 'i',\n",
       " 'of',\n",
       " 'and',\n",
       " '@',\n",
       " 'is',\n",
       " '0',\n",
       " 'that',\n",
       " '!',\n",
       " '?']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words(X_test,20)\n",
    "# [',',\n",
    "# 'the',\n",
    "# '>',\n",
    "# '.',\n",
    "# '--',\n",
    "# ':',\n",
    "# ')',\n",
    "# 'to',\n",
    "# '(',\n",
    "# 'a',\n",
    "# 'in',\n",
    "# 'i',\n",
    "# 'of',\n",
    "# 'and',\n",
    "# '@',\n",
    "# 'is',\n",
    "# '0',\n",
    "# 'that',\n",
    "# '!',\n",
    "# '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Implement to_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_hot_encoding(sentences : List[List[str]], top_words : List[str]):\n",
    "    r = np.zeros((len(sentences), len(top_words)))\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        s = set(sentence)\n",
    "        r[i] = np.array([x in s for x in top_words]).astype(float)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_hot_encoding(X[:2], top_n_words(X,10))\n",
    "#[array([1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]),\n",
    "# array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one hot encoding for the entire corpus\n",
    "top_words = top_n_words(X, 128)\n",
    "arrs = to_hot_encoding(X, top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs_test = to_hot_encoding(X_test, top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g) Set your parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n4\\n# train 0.0070419240887775155 0.994\\n# test  0.19830166383479556 0.763\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_output=True\n",
    "my_seed = 1\n",
    "my_sizes = (128,100,80,40,2)\n",
    "my_activations = (relu, tanh, tanh, sigmoid)\n",
    "my_epochs = 25\n",
    "my_chunks = 200\n",
    "'''\n",
    "4\n",
    "# train 0.0070419240887775155 0.994\n",
    "# test  0.19830166383479556 0.763\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dd/ygnrmj7170l_gqhb4wrx72lw0000gn/T/ipykernel_85765/2791676855.py:76: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.weights -= np.multiply(mu, self.gradients)\n",
      "/var/folders/dd/ygnrmj7170l_gqhb4wrx72lw0000gn/T/ipykernel_85765/2791676855.py:77: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.biases -= np.multiply(mu, self.biases_gradients)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "# train 0.2075044493435954 0.707\n",
      "# test  0.2317226905414242 0.639\n",
      "1\n",
      "# train 0.16270284141888722 0.778\n",
      "# test  0.20813140550729978 0.695\n",
      "2\n",
      "# train 0.13111274852408664 0.834\n",
      "# test  0.18138178856984397 0.731\n",
      "3\n",
      "# train 0.11681423906934035 0.85\n",
      "# test  0.17225020122105342 0.765\n",
      "4\n",
      "# train 0.09765720420192221 0.88\n",
      "# test  0.1663707397525231 0.751\n",
      "5\n",
      "# train 0.09012170811409718 0.878\n",
      "# test  0.1645646221292025 0.769\n",
      "6\n",
      "# train 0.07344157271096431 0.91\n",
      "# test  0.1637376684213452 0.765\n",
      "7\n",
      "# train 0.09900187057633103 0.867\n",
      "# test  0.18587248719818816 0.73\n",
      "8\n",
      "# train 0.09421574313192527 0.866\n",
      "# test  0.18914477584956801 0.722\n",
      "9\n",
      "# train 0.05777019420078386 0.93\n",
      "# test  0.18380299479589274 0.755\n",
      "10\n",
      "# train 0.053698305309607695 0.932\n",
      "# test  0.1741964383206386 0.766\n",
      "11\n",
      "# train 0.03682026431808623 0.962\n",
      "# test  0.18419359307920113 0.763\n",
      "12\n",
      "# train 0.03988836328748935 0.954\n",
      "# test  0.1865682714108159 0.753\n",
      "13\n",
      "# train 0.04601021694961058 0.941\n",
      "# test  0.19325684818307776 0.758\n",
      "14\n",
      "# train 0.025581404262038455 0.977\n",
      "# test  0.19980721307501148 0.739\n",
      "15\n",
      "# train 0.020088310361789107 0.978\n",
      "# test  0.1943333135636178 0.749\n",
      "16\n",
      "# train 0.016970117344130987 0.982\n",
      "# test  0.20014060218401597 0.744\n",
      "17\n",
      "# train 0.03125234006961526 0.971\n",
      "# test  0.21080292062324865 0.724\n",
      "18\n",
      "# train 0.013381416890555399 0.985\n",
      "# test  0.19648961179130525 0.745\n",
      "19\n",
      "# train 0.020239387738867114 0.979\n",
      "# test  0.21496155946885828 0.73\n",
      "20\n",
      "# train 0.01627549357463 0.981\n",
      "# test  0.19922145493393825 0.754\n",
      "21\n",
      "# train 0.009511032915712805 0.991\n",
      "# test  0.192712104102735 0.764\n",
      "22\n",
      "# train 0.008276307505349688 0.992\n",
      "# test  0.19435799047804825 0.764\n",
      "23\n",
      "# train 0.007403246561436979 0.994\n",
      "# test  0.19662855161669782 0.765\n",
      "24\n",
      "# train 0.0070419240887775155 0.994\n",
      "# test  0.19830166383479556 0.763\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(my_seed)\n",
    "myNN = MLP(my_sizes, my_activations)\n",
    "n_chunks = my_chunks\n",
    "n_epochs = my_epochs\n",
    "for i in range(n_epochs):\n",
    "    for x_chunk, t_chunk in zip(np.array_split(arrs,n_chunks), np.array_split(ts,n_chunks)):\n",
    "        myNN.update(x_chunk, t_chunk, squared_loss, 0.01)\n",
    "        # evaluate NN on entire dataset\n",
    "    if show_output:\n",
    "        print(i)\n",
    "        print(\"# train\", myNN.calc_loss(arrs,ts, squared_loss), myNN.calc_accuracy(arrs, ts))\n",
    "        print(\"# test \", myNN.calc_loss(arrs_test,ts_test, squared_loss), myNN.calc_accuracy(arrs_test, ts_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
